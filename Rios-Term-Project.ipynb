{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06956ca0-6955-41ce-bba4-6e8246b85664",
   "metadata": {},
   "source": [
    "# Classifying Password Strength Using Neural Networks\n",
    "\n",
    "*Author: Tyler Rios*\n",
    "\n",
    "*Instructor: Nikhil Krishnaswamy*\n",
    "\n",
    "*CS445: Introduction to Machine Learning*\n",
    "\n",
    "*10 May 2024*\n",
    "\n",
    "[GitHub](https://github.com/rios240/ML-Password-Assessment.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cabe5-a0de-4765-848c-708673d49c98",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Traditional Methods of Password Strength Assessment\n",
    "\n",
    "Traditionally, password strength is assessed using a set of predefined rules or metrics. These methods generally focus on the superficial attributes of a password rather than its intrinsic strength against attack methods. Common criteria include:\n",
    "\n",
    "* Length of the Password: Typically, a longer password is considered more secure. A minimum number of characters is often required.\n",
    "* Use of Special Characters: Passwords that include non-alphanumeric characters such as @, #, $, etc., are often ranked as stronger.\n",
    "* Mix of Upper and Lowercase Letters: The inclusion of both uppercase and lowercase letters is encouraged to increase password complexity.\n",
    "* Presence of Numbers: Adding numeric characters is viewed as another way to enhance password security.\n",
    "* Avoidance of Common Passwords and Patterns: Passwords that match common patterns or known weak passwords (like \"password123\" or \"123456\") are flagged as weak.\n",
    "\n",
    "While these rules are straightforward and relatively easy to implement, they have significant drawbacks. They do not account for the holistic security of a password but rather check it against a checklist. As a result, a password might meet all traditional criteria and still be vulnerable due to predictable patterns or sequences that are easy for a human or a computer algorithm to guess.\n",
    "\n",
    "### Why Use Machine Learning for Password Strength Assessment?\n",
    "\n",
    "Machine learning, particularly through the use of neural networks, offers a more nuanced approach to assessing password strength. Instead of relying on static rules, machine learning models can learn and predict based on the data from numerous password breaches and security patterns. Here are some advantages of using machine learning over traditional methods:\n",
    "\n",
    "* Pattern Recognition: Neural networks excel in recognizing complex patterns in data. In the context of passwords, they can learn to identify subtle patterns that might make a password easy to guess, even if it complies with traditional strength criteria.\n",
    "* Adaptive Learning: Unlike static rule-based systems, machine learning models can improve over time. They adapt as new types of password breaches occur and as users create new kinds of password combinations.\n",
    "* Assessment of Contextual Strength: Machine learning can evaluate not just the structure of a password but its strength relative to the most recent hacking techniques and leaked password databases.\n",
    "* Predictive Capabilities: By training on historical data, neural networks can better predict the likelihood that a given password will be compromised, thereby providing a dynamic assessment of password strength.\n",
    "\n",
    "In this notebook, we will explore how to harness the power of neural networks to create a model that assesses password strength more effectively and accurately than traditional methods. We will build and train a model that classifies passwords as either weak, moderate, or strong based on their intrinsic characteristics and learned patterns, demonstrating the potential of machine learning to enhance cybersecurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf1d9e-fadf-4052-91a6-80aca28f256c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before we can train our neural network to assess password strength, we need to preprocess the dataset to transform raw password strings into a format suitable for machine learning models. The dataset, sourced from Kaggle, categorizes passwords into three classes: weak, moderate, and strong. This classification will serve as the target labels for our training process. To prepare our data, we will encode the password strings into numerical features via ASCII encoding.\n",
    "\n",
    "### ASCII Encoding\n",
    "\n",
    "ASCII (American Standard Code for Information Interchange) encoding is a character encoding standard that represents text in computers and digital devices. Each character is assigned a unique number between 0 and 127. By converting each character of a password into its corresponding ASCII value, we create a numeric representation of the password which can be used as input for machine learning models.\n",
    "\n",
    "Steps for ASCII Encoding:\n",
    "\n",
    "1. Convert each character: Transform every character in the password to its corresponding ASCII value.\n",
    "2. Handle variable length: Since passwords can vary in length but the input to our model needs a fixed size, we will pad shorter passwords with a special value (like 0) up to a certain length.\n",
    "3. Normalize: To help the learning process, we normalize these values to ensure consistent scale across all features.\n",
    "\n",
    "### Considerations\n",
    "ASCII encoding might not differentiate between characters effectively since it only reflects the order in the character set and not the actual usage or context in passwords. In simpler terms, there is a degree of information loss with ASCII encoding. In contrast, one-hot encoding provides a more distinct representation of input features, but at the cost of higher computational overhead. In fact, when attempting to perform one-hot encoding on the dataset the amount of computational resources consumed resulted in the kernel dying. As such and despite its drawbacks, ASCII encoding will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c5add7b-dae4-49dc-b1e2-a916bb621d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "008a634f-156f-4c22-9b6c-3837a859731f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/rios240/ML-Password-Assessment/raw/main/archive.zip'\n",
    "response = requests.get(url)\n",
    "print(response.raise_for_status())\n",
    "\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zip_file.extractall('.')\n",
    "csv_filename = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f547bd1-8e13-440a-9273-0f62fb50e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "expected_delimiters = 1\n",
    "correct_lines = [line for line in lines if line.count(',') == expected_delimiters]\n",
    "\n",
    "with open(csv_filename, 'w') as file:\n",
    "    file.writelines(correct_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f5fcbc-a995-40ca-a44f-f159fa0cd08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      password  strength\n",
      "0     kzde5577         1\n",
      "1     kino3434         1\n",
      "2    visi7k1yr         1\n",
      "3     megzy123         1\n",
      "4  lamborghin1         1\n",
      "            password  strength\n",
      "669635    10redtux10         1\n",
      "669636     infrared1         1\n",
      "669637  184520socram         1\n",
      "669638     marken22a         1\n",
      "669639      fxx4pw4g         1\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv_filename)\n",
    "data.dropna(inplace=True)\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007229be-c1e1-4f97-9326-33bbef9e85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of password strengths (as percentages):\n",
      "strength\n",
      "1    74.189377\n",
      "0    13.395426\n",
      "2    12.415197\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "password_strength_counts = data['strength'].value_counts(normalize=True)\n",
    "print(\"Distribution of password strengths (as percentages):\")\n",
    "print(password_strength_counts * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "870f20e9-f0af-4008-8e24-b7215587c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d3da71-fb96-49e1-8260-24a78829762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum password length is: 220.\n",
      "0    [107, 122, 100, 101, 53, 53, 55, 55, 0, 0, 0, ...\n",
      "1    [107, 105, 110, 111, 51, 52, 51, 52, 0, 0, 0, ...\n",
      "2    [118, 105, 115, 105, 55, 107, 49, 121, 114, 0,...\n",
      "3    [109, 101, 103, 122, 121, 49, 50, 51, 0, 0, 0,...\n",
      "4    [108, 97, 109, 98, 111, 114, 103, 104, 105, 11...\n",
      "Name: ascii_encoded, dtype: object\n"
     ]
    }
   ],
   "source": [
    "max_length = data['password'].str.len().max()\n",
    "print(f'The maximum password length is: {max_length}.')\n",
    "\n",
    "def ascii_encode_padded(password, max_len):\n",
    "    ascii_values = [ord(char) for char in password]\n",
    "    padded_ascii = ascii_values + [0] * (max_len - len(ascii_values))\n",
    "    return padded_ascii\n",
    "\n",
    "data['ascii_encoded'] = data['password'].apply(ascii_encode_padded, max_len=max_length)\n",
    "\n",
    "print(data['ascii_encoded'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3fb7c82-ee9a-47fc-bbaf-471a419fa248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   char_0  char_1  char_2  char_3  char_4  char_5  char_6  char_7  char_8  \\\n",
      "0     107     122     100     101      53      53      55      55       0   \n",
      "1     107     105     110     111      51      52      51      52       0   \n",
      "2     118     105     115     105      55     107      49     121     114   \n",
      "3     109     101     103     122     121      49      50      51       0   \n",
      "4     108      97     109      98     111     114     103     104     105   \n",
      "\n",
      "   char_9  ...  char_211  char_212  char_213  char_214  char_215  char_216  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4     110  ...         0         0         0         0         0         0   \n",
      "\n",
      "   char_217  char_218  char_219  label  \n",
      "0         0         0         0      1  \n",
      "1         0         0         0      1  \n",
      "2         0         0         0      1  \n",
      "3         0         0         0      1  \n",
      "4         0         0         0      1  \n",
      "\n",
      "[5 rows x 221 columns]\n"
     ]
    }
   ],
   "source": [
    "ascii_data = pd.DataFrame(data['ascii_encoded'].tolist(), index=data.index)\n",
    "ascii_data.columns = [f'char_{i}' for i in range(ascii_data.shape[1])]\n",
    "\n",
    "ascii_data['label'] = data['strength']\n",
    "\n",
    "print(ascii_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "756c8041-9500-41c6-9484-d3d12c70f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f397409-9e5f-4dc6-81fe-a5afe6eb93fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     char_0    char_1    char_2    char_3    char_4    char_5    char_6  \\\n",
      "0  0.497585  0.324468  0.265957  0.012242  0.006424  0.006440  0.006484   \n",
      "1  0.497585  0.279255  0.292553  0.013455  0.006182  0.006318  0.006013   \n",
      "2  0.550725  0.279255  0.305851  0.012727  0.006667  0.013001  0.005777   \n",
      "3  0.507246  0.268617  0.273936  0.014788  0.014667  0.005954  0.005895   \n",
      "4  0.502415  0.257979  0.289894  0.011879  0.013455  0.013852  0.012143   \n",
      "\n",
      "     char_7    char_8    char_9  ...  char_211  char_212  char_213  char_214  \\\n",
      "0  0.006667  0.000000  0.000000  ...       0.0       0.0       0.0       0.0   \n",
      "1  0.006304  0.000000  0.000000  ...       0.0       0.0       0.0       0.0   \n",
      "2  0.014668  0.303191  0.000000  ...       0.0       0.0       0.0       0.0   \n",
      "3  0.006183  0.000000  0.000000  ...       0.0       0.0       0.0       0.0   \n",
      "4  0.012608  0.279255  0.292553  ...       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   char_215  char_216  char_217  char_218  char_219  label  \n",
      "0       0.0       0.0       0.0       0.0       0.0      1  \n",
      "1       0.0       0.0       0.0       0.0       0.0      1  \n",
      "2       0.0       0.0       0.0       0.0       0.0      1  \n",
      "3       0.0       0.0       0.0       0.0       0.0      1  \n",
      "4       0.0       0.0       0.0       0.0       0.0      1  \n",
      "\n",
      "[5 rows x 221 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "ascii_features = ascii_data.columns[ascii_data.columns.str.startswith('char_')]\n",
    "ascii_data[ascii_features] = scaler.fit_transform(ascii_data[ascii_features])\n",
    "\n",
    "print(ascii_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14936701-2798-42c7-81db-f25e7cd6f870",
   "metadata": {},
   "source": [
    "## Training a Neural Network for Password Strength Classification\n",
    "\n",
    "In this section, we will develop a neural network to classify password strengths based on their ASCII-encoded values. Our goal is to use a neural network classifier to analyze patterns in the numerical representation of passwords, transcending simple metrics like password length or the presence of special characters. Given the complexity and subtleties of password strength, neural networks are particularly well-suited for capturing nonlinear interactions between features.\n",
    "\n",
    "### Why a Neural Network?\n",
    "\n",
    "Neural networks are powerful computational models that can model complex nonlinear relationships between inputs and outputs. They consist of layers of interconnected nodes or neurons, where each connection represents a weight that is adjusted during the training process. For password strength assessment, a neural network can learn to recognize complex patterns in ASCII-encoded sequences that might indicate the robustness of a password against common attack vectors like brute force or dictionary attacks.\n",
    "\n",
    "### Addressing Data Imbalance with K-Fold Cross-Validation\n",
    "\n",
    "As scene in the Data Preprocessing section the dataset is imbalanced with varying numbers of weak, moderate, and strong passwords. To mitigate this imbalance and ensure our model generalizes well across all password strengths, we will utilize K-fold cross-validation. This technique involves dividing the dataset into 'K' subsets (or folds). The model is trained on 'K-1' folds and tested on the remaining fold, iteratively, such that each fold serves as the test set exactly once. This approach helps in assessing the model’s performance more reliably and reduces the likelihood of bias towards the more frequent classes.\n",
    "\n",
    "### Utilizing PCA for Dimensionality Reduction\n",
    "\n",
    "Given the high dimensionality of our ASCII-encoded data (with each password potentially represented by up to 220 features, one for each character's ASCII value), computational efficiency and model performance can be affected. High-dimensional spaces often suffer from the curse of dimensionality, where the volume of the space increases exponentially with the number of dimensions, making the data sparse and learning less effective.\n",
    "\n",
    "To address this, we can employ Principal Component Analysis (PCA). PCA is a statistical technique that reduces the dimensionality of the data by transforming the original variables into a new set of variables, which are linear combinations of the original variables. These new variables, called principal components, are ordered so that the first few retain most of the variation present in all of the original variables.\n",
    "\n",
    "Benefits of PCA in Our Context:\n",
    "\n",
    "* Reduction in Computational Complexity: By reducing the number of features, PCA can decrease the training time of our neural network, making it computationally more efficient.\n",
    "* Noise Reduction: PCA can help in noise reduction by eliminating variability that might be noise and retaining features that contain more signal.\n",
    "* Improved Model Performance: Reducing dimensionality with PCA might help in alleviating overfitting and improve the model’s ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d63e40-22bf-4b63-b143-f920ad5ef235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/rios240/ML-Password-Assessment/main/optimizers.py'\n",
    "response = requests.get(url)\n",
    "print(response.raise_for_status())\n",
    "\n",
    "with open('optimizers.py', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/rios240/ML-Password-Assessment/main/neuralnetworks.py'\n",
    "response = requests.get(url)\n",
    "print(response.raise_for_status())\n",
    "\n",
    "with open('neuralnetworks.py', 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d18e5627-1f26-4522-acda-4477e54466e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnetworks as nn\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce6f3466-6ed3-421d-9882-41e11b41a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_fold_cross_validation_sets(X, T, n_folds, shuffle=True):\n",
    "\n",
    "    if shuffle:\n",
    "        # Randomly order X and T\n",
    "        randorder = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randorder)\n",
    "        X = X[randorder, :]\n",
    "        T = T[randorder, :]\n",
    "\n",
    "    # Partition X and T into folds\n",
    "    n_samples = X.shape[0]\n",
    "    n_per_fold = round(n_samples / n_folds)\n",
    "    n_last_fold = n_samples - n_per_fold * (n_folds - 1)\n",
    "\n",
    "    folds = []\n",
    "    start = 0\n",
    "    for foldi in range(n_folds-1):\n",
    "        folds.append( (X[start:start + n_per_fold, :], T[start:start + n_per_fold, :]) )\n",
    "        start += n_per_fold\n",
    "    folds.append( (X[start:, :], T[start:, :]) )\n",
    "\n",
    "    # Yield k(k-1) assignments of Xtrain, Train, Xvalidate, Tvalidate, Xtest, Ttest\n",
    "\n",
    "    for validation_i in range(n_folds):\n",
    "        for test_i in range(n_folds):\n",
    "            if test_i == validation_i:\n",
    "                continue\n",
    "\n",
    "            train_i = np.setdiff1d(range(n_folds), [validation_i, test_i])\n",
    "\n",
    "            Xvalidate, Tvalidate = folds[validation_i]\n",
    "            Xtest, Ttest = folds[test_i]\n",
    "            if len(train_i) > 1:\n",
    "                Xtrain = np.vstack([folds[i][0] for i in train_i])\n",
    "                Ttrain = np.vstack([folds[i][1] for i in train_i])\n",
    "            else:\n",
    "                Xtrain, Ttrain = folds[train_i[0]]\n",
    "\n",
    "            yield Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d986b96-d769-40b1-98fc-a70deaeb0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold_cross_validation(X, T, n_folds, list_of_n_hiddens, \n",
    "                                list_of_n_epochs, list_of_learning_rates):\n",
    "    \n",
    "    results = []\n",
    "    classes = np.arange(3)\n",
    "    method = 'adam'\n",
    "    act_func = 'relu'\n",
    "    for n_epochs in list_of_n_epochs:\n",
    "        for learning_rate in list_of_learning_rates:\n",
    "            for n_hiddens in list_of_n_hiddens:\n",
    "                print(f'Running {n_hiddens} hiddens with {n_epochs} epochs and {learning_rate} learning rate.')\n",
    "\n",
    "                train_acc_list = []\n",
    "                validate_acc_list = []\n",
    "                test_acc_list = []\n",
    "\n",
    "                for Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest in generate_k_fold_cross_validation_sets(X, T, n_folds, shuffle=True):\n",
    "                    classifier = nn.NeuralNetworkClassifier(Xtrain.shape[1], n_hiddens, len(classes), activation_function=act_func)\n",
    "\n",
    "                    classifier.train(Xtrain, Ttrain, n_epochs, learning_rate, method=method, verbose=False)\n",
    "                    \n",
    "                    train_acc = np.mean(classifier.use(Xtrain)[0] == Ttrain)\n",
    "                    validate_acc = np.mean(classifier.use(Xvalidate)[0] == Tvalidate)\n",
    "                    test_acc = np.mean(classifier.use(Xtest)[0] == Ttest)\n",
    "\n",
    "                    train_acc_list.append(train_acc)\n",
    "                    validate_acc_list.append(validate_acc)\n",
    "                    test_acc_list.append(test_acc)\n",
    "\n",
    "                mean_train_acc = np.mean(train_acc_list)\n",
    "                mean_validate_acc = np.mean(validate_acc_list)\n",
    "                mean_test_acc = np.mean(test_acc_list)\n",
    "\n",
    "                results.append([n_hiddens, n_epochs, learning_rate, mean_train_acc, mean_validate_acc, mean_test_acc])\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=['Hidden Layers', 'Epochs', 'LR', 'Train Acc', 'Validate Acc', 'Test Acc'])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0a1fd0c-63b0-4876-bc7d-2f13f82d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ascii_data.drop('label', axis=1).to_numpy()\n",
    "T = ascii_data['label'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8e37d43-4ab0-4c93-8d23-14aa10510401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running [10] hiddens with 40 epochs and 0.001 learning rate.\n",
      "Running [10, 10] hiddens with 40 epochs and 0.001 learning rate.\n",
      "Running [30, 20, 10] hiddens with 40 epochs and 0.001 learning rate.\n",
      "Took 0.43 hours\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layers</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>LR</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Validate Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.659363</td>\n",
       "      <td>0.659484</td>\n",
       "      <td>0.659549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.705582</td>\n",
       "      <td>0.705495</td>\n",
       "      <td>0.705516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[30, 20, 10]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.739902</td>\n",
       "      <td>0.739838</td>\n",
       "      <td>0.739773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hidden Layers  Epochs     LR  Train Acc  Validate Acc  Test Acc\n",
       "0          [10]      40  0.001   0.659363      0.659484  0.659549\n",
       "1      [10, 10]      40  0.001   0.705582      0.705495  0.705516\n",
       "2  [30, 20, 10]      40  0.001   0.739902      0.739838  0.739773"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = run_k_fold_cross_validation(X_pca, T, 5,\n",
    "                                      [[10], [10, 10], [30, 20, 10]],\n",
    "                                      [40], [0.001])\n",
    "\n",
    "elapsed = (time.time() - start) / 60/ 60\n",
    "print(f'Took {elapsed:.2f} hours')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a37447-f4a8-4365-ae02-fc0bf20a6b4d",
   "metadata": {},
   "source": [
    "## Concluding Analysis of Neural Network Performance\n",
    "\n",
    "The results from our experiment using a neural network classifier on PCA-reduced ASCII-encoded data reveal interesting insights into the model's performance and the nature of the data used. Here we break down the implications of these results and the potential benefits of alternative encoding methods like one-hot encoding.\n",
    "Discussion of Results\n",
    "\n",
    "The table of results indicates a consistent improvement in accuracy as the complexity of the neural network increases:\n",
    "\n",
    "* The simplest model with a single layer of 10 neurons achieves an accuracy of around 65.9% across training, validation, and test datasets.\n",
    "* Expanding to two layers of 10 neurons each enhances the accuracy to about 70.5%.\n",
    "* The most complex model tested, with layers of 30, 20, and 10 neurons, respectively, shows the best performance, with accuracy reaching approximately 73.98%.\n",
    "\n",
    "These results suggest that the neural network can capture some of the patterns in the ASCII-encoded data, and that increasing the model complexity up to a point can benefit performance. However, the accuracy levels indicate that there may still be limitations due to the nature of the data encoding and the dimensionality reduction technique used.\n",
    "\n",
    "### Implications of ASCII Encoding and PCA\n",
    "\n",
    "ASCII encoding translates each character into a numerical value, providing a straightforward but somewhat crude numerical representation of passwords. This method captures the order and type of characters but may not effectively encapsulate the relationships or patterns that more directly relate to password strength, such as combinations of characters or the presence of common password phrases and structures.\n",
    "\n",
    "Using PCA on ASCII-encoded data helps to reduce dimensionality and computational load, which is crucial given the high number of features. While PCA ensures that the most informative variance is retained, it also inevitably leads to the loss of some data specifics, which might be crucial for predicting password strength more accurately.\n",
    "\n",
    "### Potential Advantages of One-Hot Encoding\n",
    "\n",
    "One-hot encoding could potentially offer improvements over ASCII encoding in the context of neural network performance for several reasons:\n",
    "\n",
    "* Feature Representation: One-hot encoding transforms each character into a binary vector where only the position corresponding to the character is marked with a 1, and all other positions are 0. This method can better represent the presence or absence of specific characters, providing a richer feature set for the neural network to learn from.\n",
    "* Data Sparsity and Network Focus: Although one-hot encoding increases data sparsity, it also allows the network to focus on the presence of specific characters without interference from numerical values of unrelated characters. This could enhance the ability of the network to learn important patterns related to password strength.\n",
    "* Handling Complexity: Neural networks are well-equipped to handle the increased dimensionality from one-hot encoding, especially with sufficient training data and computational power. The detailed features provided by one-hot encoding might enable the network to make more nuanced distinctions between different password strengths.\n",
    "\n",
    "However as discussed earlier, one-hot encoding could not be performed due to limitations in computational resources.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While the PCA-reduced ASCII encoding provided a computationally efficient approach with reasonable accuracy, the intrinsic limitations of ASCII encoding and loss of information through PCA suggest that exploring one-hot encoding could be advantageous. Future experiments should consider implementing one-hot encoding to assess whether the potential for capturing more detailed patterns in the data can translate into significantly improved model performance. This approach may require more computational resources but could lead to a more robust and nuanced understanding of password strengths, thereby enhancing predictive accuracy and cybersecurity measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4dd9d-424a-495e-ad61-49b9f93f0c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
